<!DOCTYPE html>
<html>
<head></head>
<body>

<button id="audio_to_timeseries" onclick="audio_to_timeseries()" style="display:block;">audio_to_timeseries</button>
	
<button id="view_frequency_response_from_audio" onclick="view_frequency_response_from_audio()" style="display:block;">view_frequency_response_from_audio</button>

 
<div id="data_display" style="display:block; text-align: left; width: 600px;">


 
<!-- --------------------------------------------------- -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script src='https://d3js.org/d3.v7.min.js'></script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
 
<script>

// var url = "https://interactive-examples.mdn.mozilla.net/media/cc0-audio/t-rex-roar.mp3";

var audio_information = {};

async function audio_to_timeseries() {
	
	var url = "https://interactive-examples.mdn.mozilla.net/media/cc0-audio/t-rex-roar.mp3";

  // Create audioElement
	var audioElement = await load_audio_from_url(url, mode="nocors");
  
	// -----------------------------------------------

  // Obtain audio information
	audio_information = await evaluate_audioElement();
  console.log('audio_information: ', audio_information);
  // {channels: channels, fs: fs, time_length: time_length, frameCount: frameCount, time_sample_rate_OR_timePeriod: time_sample_rate_OR_timePeriod}
	
	// -----------------------------------------------
  
	// Fetch binaryString of audio data
	var settings = {
		type : 'GET',
		async: true,
		crossDomain: true,
		xhrFields: {responseType: 'arrayBuffer'},
		dataType: 'binary',
		beforeSend: function(xhr) {xhr.withCredentials = true;},
		success: function(response) { console.log('Success'); },
		error: function(xhr, status, error) { console.log('Error:', error); }
	};
 
	var binaryString = await $.ajax(url, [,settings]).done(function(response) { return response; });
	console.log('binaryString: ', binaryString);

	// -----------------------------------------------

  // Normalize the audio data
	const normalArray_normalized = await decodeAudioData_from_binaryString_to_uint8Array(binaryString);
	const t0 = Array.from({ length: normalArray_normalized.length }, (_, i) => i * audio_information.time_sample_rate_OR_timePeriod);
	
	await plot_line_graph_dataObject(t0, normalArray_normalized, "Timeseries: both channels");
	
	// --------------------

  // Separate the channels of the audio data
  const [channel1, channel2] = await separate_stero_channels(normalArray_normalized)

	console.log('channel1.length : ', channel1.length);
	console.log('channel2.length : ', channel2.length);

	const t1 = Array.from({ length: normalArray_normalized.length }, (_, i) => i * audio_information.time_sample_rate_OR_timePeriod);

	await plot_line_graph_dataObject(t1, channel1, "Timeseries: channel 1");
	await plot_line_graph_dataObject(t1, channel2, "Timeseries: channel 2");

	// --------------------

	// Average the audio data channels
	const x_avg = await channel1.map((val, ind) => { return (val + channel2.at(ind))/2; });
	console.log('x_avg.length : ', x_avg.length);

	await plot_line_graph_dataObject(t1, x_avg, "Timeseries: channels averaged");

  return [t1, x_avg];
}

// -------------------------------------------------
  
async function audio_to_frequency_domain() {
	
	const [t1, x_avg] = await audio_to_timeseries();
  
  // --------------------
  
	// Compute frequency domain
	const freq_response0 = await DFT(x_avg);
  console.log('freq_response0.length : ', freq_response0.length);
  
  // OR
  
  const x = tf.tensor1d(x_avg);
  const freq_response1 = x.fft();
	console.log('freq_response1.length : ', freq_response1.length);
  
	var freq_arr_char = await obtain_array_characteristics(freq_response1);
	console.log('freq_arr_char: ', freq_arr_char);
  
	f = Array.from({ length: freq_response1.length }, (_, i) => i * 2 * Math.PI * 1/audio_information.time_sample_rate_OR_timePeriod);
	console.log('f.length : ', f.length);
	await plot_line_graph_dataObject(f, freq_response1, "Frequency response: Hz");
	
	const freq_response_db = freq_response1.map((val, ind) => { return 20 * Math.log10(val); });
	await plot_line_graph_dataObject(f, freq_response_db, "Frequency response: decibels");
	
	// --------------------

  // Calculate spectrogram
  var frameLength = 255;
  var frameStep = 128;

  // Convert the waveform to a spectrogram using the Short-Time Fourier Transform (STFT)
  var spectrogram = tf.signal.stft(x_avg, frameLength, frameStep);
  console.log('spectrogram : ', spectrogram);

  // Obtain the magnitude of the STFT
  spectrogram = tf.abs(spectrogram);
  console.log('spectrogram : ', spectrogram);

  // Add an extra-dimension to view the results as an image
  spectrogram = spectrogram.expandDims(0);
  console.log('spectrogram : ', spectrogram);
	
}

// -----------------------------------------------


  

// -----------------------------------------------
	
async function plot_line_graph_dataObject(x, y, title_text) {

	// const dataObject = [
        //     { x: 0, y: 10 },
        //     { x: 1, y: 20 },
        //     { x: 2, y: 15 },
        //     { x: 3, y: 25 },
        //     { x: 4, y: 18 }
        // ];

	console.log("x: ", x.slice(0, 10));
	console.log("y: ", y.slice(0, 10));
	
	var dataObject = [];
	for (var i=0; i<x.length; i++) {
		var obj = { x: x.at(i), y: y.at(i) };
		dataObject.push(obj);
	}
	
	const width = 1000;
	const height = 500;

	const margin = {top: 20, right: 30, bottom: 30, left: 40};
	
        const svg = d3.select("#data_display")
		.append("svg")
		.attr("class", 'line')
		.attr("width", width)
		.attr("height", height);

        const x_scale = d3.scaleLinear()
            .domain([0, d3.max(dataObject, d => d.x)], [margin.left, width - margin.right])
            .range([0, width]);

        const y_scale = d3.scaleLinear()
            .domain([0, d3.max(dataObject, d => d.y)], [height - margin.bottom, margin.top])
            .range([height, 0]);

        const line = d3.line()
            .x(d => x_scale(d.x))
            .y(d => y_scale(d.y));

        svg.append("path")
            .datum(dataObject)
            .attr("fill", "none")
            .attr("stroke", "steelblue")
            .attr("stroke-width", 2)
            .attr("d", line);

	// --------------------
	
	// Add the x-axis
	svg.append("g")
		.attr('class', 'x axis')
		// The next line moves the axis to the bottom
		.attr("transform", `translate(0, ${height - margin.bottom})`)
		.call(d3.axisBottom(x_scale));

	// --------------------
	
	// Add the y-axis
	svg.append("g")
		.attr('class', 'y-axis')
		.attr("transform", `translate(${margin.left}, 0)`)
		.call(d3.axisLeft(y_scale))
		// Add title
		.call(g => g.append("text")
			.attr("x", d3.max(dataObject, d => d.x)/2 )
			.attr("y", 15)
			.attr("fill", "currentColor")
			.attr("text-anchor", "middle")
			.style("font-size", "16px")
			.text(`Title: ${title_text}`));

	// --------------------

}

// -----------------------------------------------







// -------------------------------------------------
// AUDIO SUBFUNCTIONS
// -------------------------------------------------
async function play_sound_from_a_url(url) {
  
  var audioElement = load_audio_from_url(url, mode="nocors");

}

// -------------------------------------------------

async function play_sound_from_a_blob_object(blob_object) {

  var file_blob_object = new File ([blob_object], 'recorded_audio', {type: "audio/mp3"});
	var url_file_blob_object = URL.createObjectURL(file_blob_object);

  await play_sound_from_a_url(url_file_blob_object);

  URL.revokeObjectURL(url_file_blob_object);
  
}
  
// -------------------------------------------------

async function load_audio_from_url(url, mode="nocors") {

	// Create an AudioElement
	const audioElement = document.createElement('audio');
	
	audioElement.src = url;
	audioElement.id = "audio track";
	audioElement.autoplay = true;
	audioElement.setAttribute("controls", true);
	audioElement.setAttribute('preload', "auto");

	if (mode == "cors") {
		audioElement.setAttribute('crossOrigin', "anonymous");
	}

  	// Create an SourceElement (audioSourceNode)
	// var sourceElement = document.createElement('source');
	// sourceElement.setAttribute("src", url);
	// console.log('sourceElement: ', sourceElement);
  
	// audioElement.appendChild(sourceElement);
	
	document.getElementById('data_display').appendChild(audioElement);

	return audioElement;
}

// -------------------------------------------------

async function evaluate_audioElement() {

	// It appears that if the audioElement is in memory, these functions can be used

	// -----------------------------------------------
	
	// Create an AudioContext
	// var audioContext = new (window.AudioContext || window.webkitAudioContext)();
	// audioContext = new AudioContext();
	// OR
	var audioContext = new AudioContext();
 
	// -----------------------------------------------

	// Create an AnalyserNode: some signal information was more reliable with analyserNode than audioContext
	var analyserNode = audioContext.createAnalyser();
	// console.log('analyserNode: ', analyserNode);
	
	var channels = audioContext.channelCount;
	if (channels == undefined) { 
		channels = analyserNode.channelCount;
	}
	// console.log('channels: ', channels);
	
	var fs = audioContext.sampleRate;
	if (fs == undefined) { 
		fs = analyserNode.context.sampleRate; // 48000
	}
	// console.log('fs: ', fs);

	let time_length = analyserNode.context.currentTime;   // 354.976
	// console.log('time_length: ', time_length);

	// -----------------------------------------------

	const frameCount = fs * 2.0;
	
	var time_sample_rate_OR_timePeriod = (1/fs) * 1000; // every time_sample_rate there is a data point, 0.020833333333333333
	// console.log('time_sample_rate_OR_timePeriod: ', time_sample_rate_OR_timePeriod);

	// -----------------------------------------------
	
	return {channels: channels, fs: fs, time_length: time_length, frameCount: frameCount, time_sample_rate_OR_timePeriod: time_sample_rate_OR_timePeriod};
}

// -------------------------------------------------
  
  
  


// -------------------------------------------------
// AUDIO PRE-PROCESSING SUBFUNCTIONS
// -------------------------------------------------
async function decodeAudioData_from_binaryString_to_uint8Array(binaryString) {
	
	// -----------------------------------------------
	// Decode the binaryString response
	// -----------------------------------------------
	var character_array = binaryString.split('');
	console.log("character_array: ", character_array);
	// Array(38190) [ "�", "�", "�", "d", "\u0000", "\u0000", "\u0000", "\u0000", "\u0000", "\u0000", … ]
			
	// Map each [binary string character; a subset of binary string characters is UTF-8] as an [ASCII number; a number from 0 to number_of_characters]
	var byteArray = character_array.map((character) => { return character.charCodeAt(0); });
	console.log("byteArray: ", byteArray);
	// byteArray:  Array(38190) [ 65533, 65533, 65533, 100, 0, 0, 0, 0, 0, 0, … ]

	// The importance of this mapping is to limit the array values from 0 to 255.
	var uint8Array = new Uint8Array(byteArray);
	console.log('uint8Array: ', uint8Array);
	// uint8Array:  Uint8Array(38190) [ 253, 253, 253, 100, 0, 0, 0, 0, 0, 0, … ]

	// In some ways a uint8Array is an arrayBuffer because the size is "fixed" meaning that no more data will be appended to the array after the UTF-8 characters. And, it is a "fixed" array because the values of the array are limited to a certain range of numbers, from 0 to 255. 

	// Convert UTF-8 array [non-fixed length array] to a binary arrayBuffer [fixed-length array]
	const arrayBuffer = uint8Array.buffer;
	console.log('arrayBuffer: ', arrayBuffer);

	// Determine the length of the typedArray_arrayBuffer
	console.log('arrayBuffer.byteLength: ', arrayBuffer.byteLength);
	
	// --------------------
	
	// Convert the TypedArray into a normal array
	const normalArray = await Array.from(uint8Array);
	console.log('normalArray: ', normalArray);

	// Determine the length of the normalArray.length
	console.log('normalArray.length: ', normalArray.length);

	var arr_char = await obtain_array_characteristics(normalArray);
	console.log('arr_char: ', arr_char);

	// --------------------

	// Normalize the audio data in arrayBuffer from [-1, 1]
	// var normalArray_normalized = await normalArray.map((val, ind) => { return val/arr_char.max_amp; });  // gave wrong results
	// var normalArray_normalized = await normalArray.map((val, ind) => { return val/99; });  // gave wrong results
	// console.log('normalArray max min: ', [normalArray_normalized.sort().shift(), normalArray_normalized.sort().pop()])
	// RESULT: it gives a value from 0 to 2.5656... not exactly what was expected
	
	// OR

	// Try with Tensorflow library
	const tf_normalArray = tf.tensor1d(normalArray);
	const max_amp = tf.scalar(arr_char.max_amp);
	var normalArray_normalized_tf = await tf_normalArray.div(max_amp);
	  
	// --------------------
	
	// Convert Tensorflow_array to Array
	const normalArray_normalized = normalArray_normalized_tf.arraySync();
	console.log('normalArray max min: ', [normalArray_normalized.sort().shift(), normalArray_normalized.sort().pop()])
	  
	// --------------------
	
	// Only for verification
	var arr_char_normalized = await obtain_array_characteristics(normalArray_normalized);
	console.log('arr_char_normalized: ', arr_char_normalized);
	
	// --------------------

	return normalArray_normalized;
}

// -----------------------------------------------

async function separate_stero_channels(normalArray_normalized) {

  // Audio files with two channels are interleaved, meaning that [channel0_datapoint0, channel1_datapoint0, channel0_datapoint1, channel1_datapoint1, ...channel0_datapointn-1, channel1_datapointn-1] 
  var channel1 = [];
	var channel2 = normalArray_normalized.map((val, ind) => {
		if (ind % 2 == 0) {
			channel1.push(val);
			return '';
		} else {
			return val;
		}
	});
	const NonEmptyVals_toKeep = (x) => x.length != 0;
	channel2 = channel2.filter(NonEmptyVals_toKeep);
  
  return [channel1, channel2];
}

// -----------------------------------------------
  
async function obtain_array_characteristics(arr) {

	var arr_char = {};
	
	arr_char.mu = await mean(arr);
	arr_char.sigma = await std(arr);

	const arr_sort = arr.sort();
	arr_char.max_val = arr_sort.at(arr.length-1);
	arr_char.min_val = arr_sort.at(0);

	// Maximum amplitude
	arr_char.max_amp = [Math.abs(arr_char.min_val), arr_char.max_val].sort().pop();
	
	return arr_char;
}
	
// -----------------------------------------------

async function sum(arr) {
	return arr.reduce((accumulator, currentValue) => accumulator + currentValue, 0);
}

// -----------------------------------------------
  
async function mean(arr) {
	return await sum(arr)/arr.length;
}

// -----------------------------------------------

async function std(arr) {
	const mu =  await mean(arr);
	// console.log('mu: ', mu);

	var arr1 = arr.map((x) => { return x-mu; });
	const summ = await sum(arr1);
	// console.log('summ: ', summ);

	const out = Math.sqrt( Math.pow(summ, 2)/arr.length );
	// console.log('out: ', out);
	
	return out;
}

// -------------------------------------------------


async function DFT(x) {

	// Discrete Fourier Transform:  1/sqrt(N) * sum_{0}^{N-1}(x(t) exp^{-j * omega * t}), where omega = 2 * pi * k/N for k=0...N-1
	const N = x.length;
	
	var x_jomega = [];
	for (var k=0; k<N; k++) {
		var arr_at_jomega = [];
		for (var t=0; t<N; t++) {
			var omega = 2 * Math.PI * k/N;
			arr_at_jomega.push(x.at(t)*Math.exp(- omega * t));
		}
		// sum across t
		x_jomega.push(1/Math.sqrt(N) * await sum(arr_at_jomega));
	}
	return x_jomega;
}


// -------------------------------------------------

// Downsample time series to a [desired_sampleRate]

// -------------------------------------------------
  
</script>

</body>
</html>
